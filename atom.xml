<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mango</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="zengmiaokun.github.io/"/>
  <updated>2020-04-28T15:04:47.558Z</updated>
  <id>zengmiaokun.github.io/</id>
  
  <author>
    <name>MangoPro</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>中文自动分词</title>
    <link href="zengmiaokun.github.io/2020/04/19/Chinese_text_segmentation/"/>
    <id>zengmiaokun.github.io/2020/04/19/Chinese_text_segmentation/</id>
    <published>2020-04-19T08:50:00.000Z</published>
    <updated>2020-04-28T15:04:47.558Z</updated>
    
    <content type="html"><![CDATA[<p>分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。通俗地讲，中文自动分词就是利用计算机自动对中文文本中的词语进行切分，即把没有分割标志的汉字串 (没有词的边界) 转换到符合语言实际的词串即在书面汉语中建立词的边界。</p><a id="more"></a><blockquote><p><strong>例如：</strong></p><ul><li>英文：<code>I like mango</code>，计算机可以通过空格划分单词。</li><li>中文：<code>我喜欢芒果</code>，计算机不知道<code>芒</code>和<code>果</code>两个字是一个词。</li></ul><p>把中文的汉字序列切分有有意义的词就是中文分词，也成为切词。</p><p><code>我喜欢芒果</code> → <code>我</code> <code>喜欢</code> <code>芒果</code></p></blockquote><h2 id="中文分词的作用和意义"><a href="#中文分词的作用和意义" class="headerlink" title="中文分词的作用和意义"></a>中文分词的作用和意义</h2><p>在中文自然语言处理中，词是最小的能够独立活动的有意义的语言成分，单字往往没有确切的含义。中文分词是文本挖掘的基础，对于输入的一段中文，成功的进行中文分词，可以达到电脑自动识别语句含义的效果。</p><blockquote><ul><li>中文分词技术不仅仅应用于中文，还可以处理其他没有明显分隔符的文本，例如日语、韩语、以及手写英文等。</li><li>中文并不是必须的，理论上深度学习模型在样本量足够大的情况下可以不进行分词。</li></ul></blockquote><h2 id="中文自动分词的难点"><a href="#中文自动分词的难点" class="headerlink" title="中文自动分词的难点"></a>中文自动分词的难点</h2><h3 id="歧义消除"><a href="#歧义消除" class="headerlink" title="歧义消除"></a>歧义消除</h3><p>在中文分词中，如果一个待切分语句存在多个分词结果，则该语句存在切分歧义，被称为切分歧义句。待切分文本所含有的每个词如果也同样存在于分词系统词典中，则都有可能从文本中被切分出来。引起这种切分歧义的情况繁多，其中由汉字串引起的切分歧义可以划分为两种类型，即交集型切分歧义和组合型切分歧义。</p><blockquote><ul><li><p><strong>交集型歧义：</strong></p><p>交集型切分歧义指汉字串<code>ＡＢＣ</code>可以切分成<code>Ａ｜ＢＣ</code>和<code>ＡＢ｜Ｃ</code>，比如汉字串</p><p><code>吹气球</code>，有<code>吹｜气球</code>、<code>吹气｜球</code>这 两 种切分结果。</p></li><li><p><strong>组合型歧义：</strong></p><p>组合型切分歧义指汉字串<code>ＡＢ</code>可以作为整体不切分，也可以切分成<code>Ａ｜Ｂ</code>，比如<code>中国人民</code>，可以不切分，也可以切分为<code>中国｜人民</code>。</p></li><li><p><strong>交叉型歧义：</strong></p><p>交叉型歧义指交集型歧义和组合型歧义同时出现。</p></li></ul></blockquote><h3 id="未登录词识别"><a href="#未登录词识别" class="headerlink" title="未登录词识别"></a>未登录词识别</h3><p>命名实体(人名、地名)、新词，专业术语称为未登录词。也就是那些在分词词典中没有收录，但又确实能称为词的那些词。最典型的是人名，人可以很容易理解，但要是让计算机去识别就很困难。</p><p>除了人名以外，还有机构名、地名、产品名、商标名、简称、省略语等都是很难处理的问题，而且这些又正好是人们经常使用的词，因此分词系统中的新词识别十分重要。新词识别准确率已经成为评价一个分词系统好坏的重要标志之一。</p><h2 id="中文自动分词的主要方法"><a href="#中文自动分词的主要方法" class="headerlink" title="中文自动分词的主要方法"></a>中文自动分词的主要方法</h2><h3 id="基于词典的分词"><a href="#基于词典的分词" class="headerlink" title="基于词典的分词"></a>基于词典的分词</h3><p>这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个”充分大的“机器词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功(识别出一个词)。按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大(最长)匹配和最小(最短)匹配；常用的几种机械分词方法如下：</p><ol><li>正向最大匹配法(由左到右的方向)；</li><li>逆向最大匹配法(由右到左的方向)</li><li>最少切分(使每一句中切出的词数最小)</li><li>双向最大匹配法(进行由左到右、由右到左两次扫描)</li></ol><p>这类算法的优点是<strong>速度快</strong>，实现简单，效果尚可；但对<strong>歧义</strong>和<strong>未登录词</strong>处理效果不佳。</p><blockquote><ul><li><p>上述各种方法之间可以相互结合，例如可以将正向最大匹配方法和逆向最大匹配方法结合起来构成双向匹配法。</p></li><li><p>由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很少使用。</p></li><li><p>一般说来，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少。</p></li></ul></blockquote><h3 id="基于统计的分词"><a href="#基于统计的分词" class="headerlink" title="基于统计的分词"></a>基于统计的分词</h3><p>从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度。可以对语料中相邻共现的各个字的组合的频度进行统计，计算它们的互现信息。定义两个字的互现信息，计算两个汉字X、Y的相邻共现概率。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成了一个词。这种方法只需对语料中的<strong>字组频度</strong>进行统计，<strong>不需要切分词典</strong>，因而又叫做无词典分词法或统计取词方法。</p><p>统计分词算法通常可分为<strong>基于有向图的全切分算法</strong>和<strong>基于字标注的机器学习算法</strong>：</p><ul><li>基于有向图的算法是将目标文本中的字符作为节点构成有向无环图，并利用统计学方法对分词路径进行选择 。</li><li>基于字标注的分词算法是一种基于构词方法的分词算法，通过对文本中的每个字符的词位进行标注完成分词，可以<strong>有效识别未登录词</strong>。</li></ul><p>该算法的缺点是会经常抽出一些共现频度高、但并不是词的常用字组。例如”这一”、”之一”、”有的”、”我的”、”许多的”等。此外该算法对常用词的识别精度差，时空开销大，所以常与基于词典的分词方法搭配使用，取长补短。</p><blockquote><p>主要的<strong>统计模型</strong>有：<strong>N 元文法模型</strong>（N-gram），<strong>隐马尔可夫模型</strong>（Hidden Markov Model ，HMM），<strong>最大熵模型</strong>（ME），<strong>条件随机场模型</strong>（Conditional Random Fields，CRF）等。</p></blockquote><h3 id="基于理解的分词"><a href="#基于理解的分词" class="headerlink" title="基于理解的分词"></a>基于理解的分词</h3><p>这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来<strong>处理歧义</strong>现象。它通常包括三个部分：<strong>分词子系统</strong>、<strong>句法语义子系统</strong>、<strong>总控部分</strong>。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还<strong>处在试验阶段</strong>。</p><h2 id="中文自动分词的开源项目"><a href="#中文自动分词的开源项目" class="headerlink" title="中文自动分词的开源项目"></a>中文自动分词的开源项目</h2><h3 id="结巴-Jieba-分词"><a href="#结巴-Jieba-分词" class="headerlink" title="结巴(Jieba)分词"></a>结巴(Jieba)分词</h3><h3 id="HanLP分词器"><a href="#HanLP分词器" class="headerlink" title="HanLP分词器"></a>HanLP分词器</h3><h3 id="清华大学THULAC"><a href="#清华大学THULAC" class="headerlink" title="清华大学THULAC"></a>清华大学THULAC</h3><h3 id="北京大学PkuSeg"><a href="#北京大学PkuSeg" class="headerlink" title="北京大学PkuSeg"></a>北京大学PkuSeg</h3><h3 id="SnowNLP"><a href="#SnowNLP" class="headerlink" title="SnowNLP"></a>SnowNLP</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。通俗地讲，中文自动分词就是利用计算机自动对中文文本中的词语进行切分，即把没有分割标志的汉字串 (没有词的边界) 转换到符合语言实际的词串即在书面汉语中建立词的边界。&lt;/p&gt;
    
    </summary>
    
    
      <category term="自然语言处理" scheme="zengmiaokun.github.io/categories/nlp/"/>
    
    
      <category term="自然语言处理" scheme="zengmiaokun.github.io/tags/nlp/"/>
    
  </entry>
  
</feed>
